{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f15afcf-b8df-4dc4-b5bd-401ca429bdce",
   "metadata": {},
   "source": [
    "# Introduction to OpenSearch\n",
    "\n",
    "OpenSearch is search engine that nicely scales to billion size documents. Its indexes can be composed of multiple fields, each one indexing different parts of the documents. Each field has its own data type, e.g., text, keywords, numbers, knn-vectors. Text fields use a specific analyser and a retrieval model.\n",
    "\n",
    "A server is available on the cluster for this course. You can also set up your own server in your local machine. Docker is a convenient solution: https://opensearch.org/docs/latest/opensearch/install/docker/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74452b2-5e1a-4eb5-9cdd-fd9bc3a3603d",
   "metadata": {},
   "source": [
    "## OpenSearch connection settings\n",
    "\n",
    "For this course, a server is available on the cluster for this course. If you really need to you can set up your own server in your local machine. I advise you to use docker: https://opensearch.org/docs/latest/opensearch/install/docker/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccb245c-8503-4819-a30c-8ac9e9c6f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "import requests\n",
    "\n",
    "host = '10.10.255.202'\n",
    "port = 8200\n",
    "user = '' # Add your user name here.\n",
    "password = '' # Add your user password here. For testing only. Don't store credentials in code. \n",
    "index_name = user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b4b34-d8bb-4915-ba1c-501fbba2c7fc",
   "metadata": {},
   "source": [
    "## OpenSearch Python API \n",
    "\n",
    "OpenSearch communicates via REST which can be accessed with CURL (or its Python port, the requests library). For your conveninence we will use the Python client. A short introduction is available here:\n",
    "https://opensearch.org/docs/latest/clients/python/\n",
    "\n",
    "## Opening and closing a connection\n",
    "The example below establishes a connection with the server and, if your index is already created, it displays the index settings, mappings and the number of indexed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b3f988-477d-4cb8-816b-70b230d7a55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True, 'shards_acknowledged': True}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX SETTINGS\n",
      "{'user219': {'settings': {'index': {'creation_date': '1648455050590',\n",
      "                                    'knn': 'true',\n",
      "                                    'number_of_replicas': '0',\n",
      "                                    'number_of_shards': '4',\n",
      "                                    'provided_name': 'user219',\n",
      "                                    'refresh_interval': '1s',\n",
      "                                    'uuid': 'gtX44npLTmiiAFtC7MjeEw',\n",
      "                                    'version': {'created': '135238227'}}}}}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX MAPPINGS\n",
      "{'user219': {'mappings': {'properties': {'contents': {'analyzer': 'standard',\n",
      "                                                      'similarity': 'BM25',\n",
      "                                                      'type': 'text'},\n",
      "                                         'doc_id': {'type': 'keyword'},\n",
      "                                         'tags': {'fields': {'keyword': {'ignore_above': 256,\n",
      "                                                                         'type': 'keyword'}},\n",
      "                                                  'type': 'text'}}}}}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX #DOCs\n",
      "{'count': 2, '_shards': {'total': 4, 'successful': 4, 'skipped': 0, 'failed': 0}}\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy import helpers\n",
    "\n",
    "# Optional client certificates if you don't want to use HTTP basic authentication.\n",
    "# client_cert_path = '/full/path/to/client.pem'\n",
    "# client_key_path = '/full/path/to/client-key.pem'\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (user, password),\n",
    "    # client_cert = client_cert_path,\n",
    "    # client_key = client_key_path,\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    "    #, ca_certs = ca_certs_path\n",
    ")\n",
    "\n",
    "if client.indices.exists(index_name):\n",
    "\n",
    "    resp = client.indices.open(index = index_name)\n",
    "    print(resp)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX SETTINGS')\n",
    "    settings = client.indices.get_settings(index = index_name)\n",
    "    pp.pprint(settings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX MAPPINGS')\n",
    "    mappings = client.indices.get_mapping(index = index_name)\n",
    "    pp.pprint(mappings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX #DOCs')\n",
    "    print(client.count(index = index_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdef48c-21cf-4869-9aa8-64788c610740",
   "metadata": {},
   "source": [
    "To release resources in the OpenSearch server you should always close the index handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29117f92-f724-4fe4-a148-cf19b56dd53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True, 'shards_acknowledged': True, 'indices': {'user219': {'closed': True}}}\n"
     ]
    }
   ],
   "source": [
    "resp = client.indices.close(index = index_name, timeout=\"600s\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74203094-61c8-4f9f-9372-259cba0474a1",
   "metadata": {},
   "source": [
    "# Index creation and configuration\n",
    "In this section we will see how to create an index, inspect the configuration and delete an index if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec20622-378e-43a1-8603-02b3eaf1df90",
   "metadata": {},
   "source": [
    "## Create an index with your own settings\n",
    "\n",
    "Lets first create an index distributed across 4 shards, no replicas, and with support for knn-vector data types. In terms of indexed data, we define two data properties: doc_id and contents, with data types keyword and text respectively.\n",
    "\n",
    "Property type | Description\n",
    "-----|-----\n",
    "text|A string sequence of characters that represent full-text values.\n",
    "keyword|A string sequence of structured characters, such as an email or ZIP code.\n",
    "boolean|OpenSearch accepts true and false as boolean values. An empty string is equal to false. \n",
    "integer|A signed 32-bit number. \n",
    "float|A single-precision 32-bit floating point number. \n",
    "double|A double-precision 64-bit floating point number.\n",
    "date|if new string fields match a dateâ€™s format, then the string is processed as a date field. For example, date: \"2012/03/11\" is processed as a date.\n",
    "objects|Objects are standard JSON objects, which can have fields and mappings of their own. For example, a movies object can have additional properties such as title, year, and director.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06559069-2ee1-4c20-ac09-802fd65bdd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already existed. Nothing to be done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index_body = {\n",
    "   \"settings\":{\n",
    "      \"index\":{\n",
    "         \"number_of_replicas\":0,\n",
    "         \"number_of_shards\":4,\n",
    "         \"refresh_interval\":\"-1\",\n",
    "         \"knn\":\"true\"\n",
    "      }\n",
    "   },\n",
    "   \"mappings\":{\n",
    "       \"dynamic\":      \"strict\",\n",
    "       \"properties\":{\n",
    "         \"doc_id\":{\n",
    "            \"type\":\"keyword\"\n",
    "         },\n",
    "         \"tags\":{\n",
    "            \"type\":\"keyword\"\n",
    "         },\n",
    "         \"contents\":{\n",
    "            \"type\":\"text\",\n",
    "            \"analyzer\":\"standard\",\n",
    "            \"similarity\":\"BM25\"\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already existed. Nothing to be done.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index:')\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926fc1ba-4906-460e-9c35-926acf3d24ec",
   "metadata": {},
   "source": [
    "## Check the indexes, settings and mappings\n",
    "Once you create an index, you should verify that it is created according to your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dcd4df4-5a77-44fd-b62e-b3dcac013e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------- INDEX SETTINGS\n",
      "{'user219': {'settings': {'index': {'creation_date': '1648455050590',\n",
      "                                    'knn': 'true',\n",
      "                                    'number_of_replicas': '0',\n",
      "                                    'number_of_shards': '4',\n",
      "                                    'provided_name': 'user219',\n",
      "                                    'refresh_interval': '1s',\n",
      "                                    'uuid': 'gtX44npLTmiiAFtC7MjeEw',\n",
      "                                    'version': {'created': '135238227'}}}}}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX MAPPINGS\n",
      "{'user219': {'mappings': {'properties': {'contents': {'analyzer': 'standard',\n",
      "                                                      'similarity': 'BM25',\n",
      "                                                      'type': 'text'},\n",
      "                                         'doc_id': {'type': 'keyword'},\n",
      "                                         'tags': {'fields': {'keyword': {'ignore_above': 256,\n",
      "                                                                         'type': 'keyword'}},\n",
      "                                                  'type': 'text'}}}}}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX #DOCs\n",
      "{'count': 2, '_shards': {'total': 4, 'successful': 4, 'skipped': 0, 'failed': 0}}\n"
     ]
    }
   ],
   "source": [
    "print('\\n----------------------------------------------------------------------------------- INDEX SETTINGS')\n",
    "index_settings = {\n",
    "    \"settings\":{\n",
    "      \"index\":{\n",
    "         \"refresh_interval\" : \"1s\"\n",
    "      }\n",
    "   }\n",
    "}\n",
    "client.indices.put_settings(index = index_name, body = index_settings)\n",
    "settings = client.indices.get_settings(index = index_name)\n",
    "pp.pprint(settings)\n",
    "\n",
    "print('\\n----------------------------------------------------------------------------------- INDEX MAPPINGS')\n",
    "mappings = client.indices.get_mapping(index = index_name)\n",
    "pp.pprint(mappings)\n",
    "\n",
    "print('\\n----------------------------------------------------------------------------------- INDEX #DOCs')\n",
    "print(client.count(index = index_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c8045-9117-4e45-831d-dd52b4b69581",
   "metadata": {},
   "source": [
    "## Index deletion\n",
    "Be absolutely sure tha you want to delete the index. There is no way of recovering it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd757fe9-d568-4ed8-84cf-76e568a7e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting index:\n",
      "{'acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "be absolutely sure that you want to comment this line and actually delete the index!!!\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    # Delete the index.\n",
    "    response = client.indices.delete(\n",
    "        index = index_name,\n",
    "        timeout = \"600s\"\n",
    "    )\n",
    "    print('\\nDeleting index:')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32f953-2533-4e93-a7c0-4a0470e1f32c",
   "metadata": {},
   "source": [
    "# Document processing and indexing\n",
    "\n",
    "## Built-in document tokenizers and analyzers\n",
    "\n",
    "The built-in tokenizers and analyzers include: standard, simple, whitespace, stop, keyword, pattern, [language], fingerprint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab482d99-c852-44e8-a79e-8bd2cadf31c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'token': 'the',\n",
       "   'start_offset': 0,\n",
       "   'end_offset': 3,\n",
       "   'type': 'word',\n",
       "   'position': 0},\n",
       "  {'token': 'quick',\n",
       "   'start_offset': 4,\n",
       "   'end_offset': 9,\n",
       "   'type': 'word',\n",
       "   'position': 1},\n",
       "  {'token': 'brown',\n",
       "   'start_offset': 10,\n",
       "   'end_offset': 15,\n",
       "   'type': 'word',\n",
       "   'position': 2},\n",
       "  {'token': 'fox',\n",
       "   'start_offset': 16,\n",
       "   'end_offset': 19,\n",
       "   'type': 'word',\n",
       "   'position': 3}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anls = {\n",
    "  \"analyzer\": \"whitespace\",\n",
    "  \"text\": \"the quick brown fox\"\n",
    "}\n",
    "client.indices.analyze(body=anls, index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92db291d-f75b-4dd4-8256-757053b4cf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'token': 'the',\n",
       "   'start_offset': 0,\n",
       "   'end_offset': 3,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 0},\n",
       "  {'token': 'quick',\n",
       "   'start_offset': 4,\n",
       "   'end_offset': 9,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 1},\n",
       "  {'token': 'brown',\n",
       "   'start_offset': 10,\n",
       "   'end_offset': 15,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 2},\n",
       "  {'token': 'fox',\n",
       "   'start_offset': 16,\n",
       "   'end_offset': 19,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 3}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anls = {\n",
    "  \"analyzer\": \"standard\",\n",
    "  \"text\": \"the quick brown fox\"\n",
    "}\n",
    "client.indices.analyze(body=anls, index=index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ab7be-314f-453d-b566-534e7c04e338",
   "metadata": {},
   "source": [
    "\n",
    "## Simple document indexing\n",
    "\n",
    "You can index documents in OpenSearch by adding one document at a time. This is done with JSON data as follows. Note that the id parameter in the API call is the unique identifier, which works as a key.\n",
    "\n",
    "If you add one document that already exists in the index, it will update the data. You can update only part of the fields and leave all the others unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34de6117-04a3-4eb0-b182-fcc66fed830e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created\n",
      "created\n"
     ]
    }
   ],
   "source": [
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "doc = {\n",
    "    'doc_id': 'documentA',\n",
    "    'tags': ['red', 'blue'],\n",
    "    'contents': docs[0]\n",
    "}\n",
    "resp = client.index(index=index_name, id=1, body=doc)\n",
    "print(resp['result'])\n",
    "\n",
    "doc = {\n",
    "    'doc_id': 'documentB',\n",
    "    'tags': ['red'],\n",
    "    'contents': docs[1]\n",
    "}\n",
    "resp = client.index(index=index_name, id=2, body=doc)\n",
    "print(resp['result'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c36e16-5fd4-4c6e-806a-30a1bac5ee46",
   "metadata": {},
   "source": [
    "## Deleting a single document\n",
    "Similarly, you can delete one document at a time as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04018f-2ed8-49c0-a901-d4d9877a8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "Delete the document.\n",
    "\n",
    "response = client.delete(\n",
    "    index = index_name,\n",
    "    id = id\n",
    ")\n",
    "\n",
    "print('\\nDeleting document:')\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db83cf-94fd-4416-8438-3d15f2899928",
   "metadata": {},
   "source": [
    "# Search methods\n",
    "\n",
    "OpenSearch supports many different search methods and has its own Query Syntax Language covering a wide range of search options:\n",
    "\n",
    "https://opensearch.org/docs/latest/opensearch/query-dsl/index/\n",
    "\n",
    "## Text-based search\n",
    "\n",
    "OpenSearch is one of the best solutions for searching text. The text-based search documentation is available here:\n",
    "\n",
    "https://opensearch.org/docs/latest/opensearch/query-dsl/full-text/\n",
    "\n",
    "In the example below the 'query'  parameter indicates the search query, the 'size' parameter indicates the number of documents to be returned, the parametner 'source' indicates which fields should be returned in the search results, and the 'fields' parameter indicates the list of fields to be searched. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a2e93-3f41-4ac7-b2cd-84b66f999597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5beaf96-5c80-4853-a7ac-20784a633f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search results:\n",
      "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 4, 'total': 4},\n",
      " 'hits': {'hits': [{'_id': '1',\n",
      "                    '_index': 'user219',\n",
      "                    '_score': 2.2617629,\n",
      "                    '_source': {},\n",
      "                    '_type': '_doc'},\n",
      "                   {'_id': '2',\n",
      "                    '_index': 'user219',\n",
      "                    '_score': 0.18232156,\n",
      "                    '_source': {},\n",
      "                    '_type': '_doc'}],\n",
      "          'max_score': 2.2617629,\n",
      "          'total': {'relation': 'eq', 'value': 2}},\n",
      " 'timed_out': False,\n",
      " 'took': 3}\n"
     ]
    }
   ],
   "source": [
    "qtxt = \"How many people live in London?\"\n",
    "\n",
    "query_bm25 = {\n",
    "  'size': 5,\n",
    "  '_source': ['_tags'],\n",
    "#  '_source': ['doc_id'],\n",
    "#  '_source': '',\n",
    "  'query': {\n",
    "    'multi_match': {\n",
    "      'query': qtxt,\n",
    "      'fields': ['contents']\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query_bm25,\n",
    "    index = index_name\n",
    ")\n",
    "\n",
    "print('\\nSearch results:')\n",
    "pp.pprint(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c209dd3-9ee3-4eed-86aa-64f14d9f2ec3",
   "metadata": {},
   "source": [
    "\n",
    "## Term queries\n",
    "\n",
    "https://opensearch.org/docs/latest/opensearch/query-dsl/term/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62cd4f52-56f5-4940-a152-4bc9914c3fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search results:\n",
      "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 4, 'total': 4},\n",
      " 'hits': {'hits': [{'_id': '2',\n",
      "                    '_index': 'user219',\n",
      "                    '_score': 0.21110919,\n",
      "                    '_source': {'contents': 'London is known for its financial '\n",
      "                                            'district'},\n",
      "                    '_type': '_doc'},\n",
      "                   {'_id': '1',\n",
      "                    '_index': 'user219',\n",
      "                    '_score': 0.160443,\n",
      "                    '_source': {'contents': 'Around 9 Million people live in '\n",
      "                                            'London'},\n",
      "                    '_type': '_doc'}],\n",
      "          'max_score': 0.21110919,\n",
      "          'total': {'relation': 'eq', 'value': 2}},\n",
      " 'timed_out': False,\n",
      " 'took': 3}\n"
     ]
    }
   ],
   "source": [
    "qtxt = \"How many people live in London?\"\n",
    "\n",
    "query_bm25 = {\n",
    "  'size': 5,\n",
    "  '_source': ['contents'],\n",
    "  'query': {\n",
    "        \"term\": {\n",
    "            \"tags\" : 'red'\n",
    "        }\n",
    "   }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query_bm25,\n",
    "    index = index_name\n",
    ")\n",
    "\n",
    "print('\\nSearch results:')\n",
    "pp.pprint(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c4573-1bd0-471a-ad6d-b51c85b0498f",
   "metadata": {},
   "source": [
    "\n",
    "## Boolean queries\n",
    "\n",
    "https://opensearch.org/docs/latest/opensearch/query-dsl/bool/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c16eb60-d108-4266-82ab-d9b762f3d809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search results:\n",
      "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 4, 'total': 4},\n",
      " 'hits': {'hits': [],\n",
      "          'max_score': None,\n",
      "          'total': {'relation': 'eq', 'value': 0}},\n",
      " 'timed_out': False,\n",
      " 'took': 2}\n"
     ]
    }
   ],
   "source": [
    "qtxt = \"How many people live in London?\"\n",
    "\n",
    "query_bm25 = {\n",
    "  'size': 5,\n",
    "  '_source': ['contents'],\n",
    "#  '_source': ['doc_id'],\n",
    "#  '_source': '',\n",
    "  'query': {\n",
    "      'bool':{\n",
    "          \"must\":{\n",
    "            \"term\": {\n",
    "                \"tags\" : 'red'\n",
    "            }\n",
    "          },\n",
    "          \"should\": \n",
    "        {\n",
    "            'multi_match': {\n",
    "              'query': qtxt,\n",
    "              'fields': ['contents']\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query_bm25,\n",
    "    index = index_name\n",
    ")\n",
    "\n",
    "print('\\nSearch results:')\n",
    "pp.pprint(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baddea7-76ac-47b0-b071-250cb7014c9f",
   "metadata": {},
   "source": [
    "# Approximate KNN search in semantic embeddings\n",
    "\n",
    "## Create an index with dense vectors\n",
    "\n",
    "To create the dense vector field you can use the configuration provided below. The 'dimension' property indicates the dimensionality of the indexed vectors, the 'space_type' indicates similarity function, and the parameters are specific to the indexing method, the 'hnsw' (Hierarichical Navigable Small World).\n",
    "\n",
    "For details see the https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a8f5c-a6df-4b5a-ba88-55dafba4682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_body = {\n",
    "   \"settings\":{\n",
    "      \"index\":{\n",
    "         \"number_of_replicas\":0,\n",
    "         \"number_of_shards\":4,\n",
    "         \"refresh_interval\":\"-1\",\n",
    "         \"knn\":\"true\"\n",
    "      }\n",
    "   },    \n",
    "   \"mappings\":{\n",
    "      \"dynamic\":      \"strict\",\n",
    "      \"properties\":{\n",
    "         \"doc_id\":{\n",
    "            \"type\":\"keyword\"\n",
    "         },\n",
    "         \"contents\":{\n",
    "            \"type\":\"text\",\n",
    "            \"analyzer\": \"standard\",\n",
    "#            \"analyzer\":\"my_analyzer\",\n",
    "            \"similarity\":\"BM25\"\n",
    "         },\n",
    "         \"sentence_embedding\":{\n",
    "            \"type\":\"knn_vector\",\n",
    "            \"dimension\": 768,\n",
    "            \"method\":{\n",
    "               \"name\":\"hnsw\",\n",
    "               \"space_type\":\"innerproduct\",\n",
    "               \"engine\":\"faiss\",\n",
    "               \"parameters\":{\n",
    "                  \"ef_construction\":256,\n",
    "                  \"m\":48\n",
    "               }\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already existed. You may force the new mappings.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index:')\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ace82-ea25-4603-a69b-cf819e620283",
   "metadata": {},
   "source": [
    "## Dual-Encoders\n",
    "\n",
    "To compute the embedding vectors of each document, we can use the Transformer encodres trained in the MSMARCO Dataset. There are many other models available in the HuggingFace repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28536391-5b9b-41b9-897b-6e353fd0c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-v2\")\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "doc_emb = encode(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea19c55-e60f-4c3e-8444-a9837cbf1e94",
   "metadata": {},
   "source": [
    "## Indexing document embedding vectors\n",
    "\n",
    "In the previous step we saw how to compute the embedding representation of a document. You can index document embeddings in OpenSearch by adding a new field to your JSON file as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f65f4b3b-a06f-4654-8985-23c51d5bb6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created\n",
      "created\n"
     ]
    }
   ],
   "source": [
    "doc = {\n",
    "    'doc_id': 'documentA',\n",
    "    'contents': docs[0],\n",
    "    'sentence_embedding': doc_emb[0].numpy()\n",
    "}\n",
    "resp = client.index(index=index_name, id=1, body=doc)\n",
    "print(resp['result'])\n",
    "\n",
    "doc = {\n",
    "    'doc_id': 'documentB',\n",
    "    'contents': docs[1],\n",
    "    'sentence_embedding': doc_emb[1].numpy()\n",
    "}\n",
    "resp = client.index(index=index_name, id=2, body=doc)\n",
    "print(resp['result'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df3cb7-927f-40ac-9b46-a75e4fc9a9c1",
   "metadata": {},
   "source": [
    "## Embedding spaces search\n",
    "\n",
    "Similarly, you need to compute the embedding representation of the query and submit it in the search query as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3f2f2bf-7513-46cc-8dc1-4e7acbf4bf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search results:\n",
      "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 4, 'total': 4},\n",
      " 'hits': {'hits': [{'_id': '1',\n",
      "                    '_index': 'user219',\n",
      "                    '_score': 1.9538686,\n",
      "                    '_source': {'doc_id': 'documentA'},\n",
      "                    '_type': '_doc'},\n",
      "                   {'_id': '2',\n",
      "                    '_index': 'user219',\n",
      "                    '_score': 1.434623,\n",
      "                    '_source': {'doc_id': 'documentB'},\n",
      "                    '_type': '_doc'}],\n",
      "          'max_score': 1.9538686,\n",
      "          'total': {'relation': 'eq', 'value': 2}},\n",
      " 'timed_out': False,\n",
      " 'took': 27}\n"
     ]
    }
   ],
   "source": [
    "# Compute the query embedding\n",
    "query = \"How many people live in London?\"\n",
    "query_emb = encode(query)\n",
    "\n",
    "query_denc = {\n",
    "  'size': 5,\n",
    "#  '_source': ['doc_id', 'contents', 'sentence_embedding'],\n",
    "#  '_source': ['doc_id', 'contents'],\n",
    "  '_source': ['doc_id'],\n",
    "   \"query\": {\n",
    "        \"knn\": {\n",
    "          \"sentence_embedding\": {\n",
    "            \"vector\": query_emb[0].numpy(),\n",
    "            \"k\": 2\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query_denc,\n",
    "    index = index_name\n",
    ")\n",
    "\n",
    "print('\\nSearch results:')\n",
    "pp.pprint(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e7503-ad00-4660-80bc-ab3d19767f94",
   "metadata": {},
   "source": [
    "## Training dual-encoders\n",
    "\n",
    "You can fine-tune dual-encoders in your domain data and get some extra points out of your search architecture.\n",
    "\n",
    "      https://www.sbert.net/docs/training/overview.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3147e3-d12b-49de-a424-52c6e46adcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "#Define your train examples. You need more than just two examples...\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "\n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafcf2d6-ce0a-499e-a4cf-f5831b8f1bdf",
   "metadata": {},
   "source": [
    "# Specialized document analyzers\n",
    "Text data can be represented in different ways. In this section we will see different ways of transforming natural language documents into a computational representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7952cf50-2bc1-464a-a4e1-5f6374ceb9c5",
   "metadata": {},
   "source": [
    "## Create an index with specific analyzers\n",
    "The built-in analyzers offer a wide range of text processing methods. Each field can have its own analyzer and users can also define their own analyzers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a50aa7b-f26b-4922-b9d7-671b1a631f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'user220'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index_body = {\n",
    "   \"settings\":{\n",
    "      \"index\":{\n",
    "         \"number_of_replicas\":0,\n",
    "         \"number_of_shards\":4,\n",
    "         \"refresh_interval\":\"-1\",\n",
    "         \"knn\":\"true\"\n",
    "      },\n",
    "      \"analysis\":{\n",
    "         \"filter\":{\n",
    "            \"edge_ngram_filter\":{\n",
    "               \"type\":\"edge_ngram\",\n",
    "               \"min_gram\":1,\n",
    "               \"max_gram\":20\n",
    "            }\n",
    "         },\n",
    "         \"analyzer\":{\n",
    "            \"my_analyzer\":{\n",
    "               \"type\":\"custom\",\n",
    "               \"tokenizer\":\"standard\",\n",
    "               \"filter\":[\n",
    "                  \"lowercase\",\n",
    "                  \"edge_ngram_filter\"\n",
    "               ]\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "   },\n",
    "   \"mappings\":{\n",
    "      \"properties\":{\n",
    "         \"doc_id\":{\n",
    "            \"type\":\"keyword\"\n",
    "         },\n",
    "         \"contents\":{\n",
    "            \"type\":\"text\",\n",
    "            \"analyzer\":\"my_analyzer\",\n",
    "            \"similarity\":\"BM25\"\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already existed. Nothing to be done.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index:')\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03fe12f-bea1-40a2-9e70-337108a38038",
   "metadata": {},
   "source": [
    "## Spacy: Tokens, lemmas and POS\n",
    "If the built-in text processing methods are not sufficient for your problem, you can use external libraries like Spacy to extract other representations of text, such as POS and lemmas, or to use other stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d6fa007-c5f2-4a22-bee0-61d84eafc932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token      lemma      pos    tag    dep        shape      alpha stop\n",
      "------------------------------------------------------------------------------\n",
      "Apple      Apple      PROPN  NNP    nsubj      Xxxxx      True False\n",
      "is         be         AUX    VBZ    aux        xx         True True\n",
      "looking    look       VERB   VBG    ROOT       xxxx       True False\n",
      "at         at         ADP    IN     prep       xx         True True\n",
      "buying     buy        VERB   VBG    pcomp      xxxx       True False\n",
      "U.K.       U.K.       PROPN  NNP    compound   X.X.       False False\n",
      "startup    startup    NOUN   NN     dobj       xxxx       True False\n",
      "for        for        ADP    IN     prep       xxx        True True\n",
      "$          $          SYM    $      quantmod   $          False False\n",
      "1          1          NUM    CD     compound   d          False False\n",
      "billion    billion    NUM    CD     pobj       xxxx       True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "save_figures = False\n",
    "\n",
    "print(\"token\".ljust(10), \"lemma\".ljust(10), \"pos\".ljust(6), \"tag\".ljust(6), \"dep\".ljust(10),\n",
    "            \"shape\".ljust(10), \"alpha\", \"stop\")\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "for token in doc:\n",
    "    print(token.text.ljust(10), token.lemma_.ljust(10), token.pos_.ljust(6), token.tag_.ljust(6), token.dep_.ljust(10),\n",
    "            token.shape_.ljust(10), token.is_alpha, token.is_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef39997-8165-4514-9fd5-9716f15893eb",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "Spacy also identifies the mentions of relevant named entities. This can have a number of applications, such as selecting the documents that mention a given named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "287df26e-fe92-438c-89ad-15f409a8e7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple        ORG        0 5\n",
      "U.K.         GPE        27 31\n",
      "$1 billion   MONEY      44 54\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text.ljust(12), ent.label_.ljust(10), ent.start_char, ent.end_char)\n",
    "\n",
    "html_ent = displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d69120-650d-4daf-9223-eaa07d299bd7",
   "metadata": {},
   "source": [
    "# Status admin requests\n",
    "\n",
    "You need to have admin permisions to run the commands below. This is useful if you use your own instalation of OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a2670f-6c07-49eb-878c-1a46aabe50d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy import helpers\n",
    "\n",
    "host = '10.10.255.202'\n",
    "port = 8200\n",
    "user = ''\n",
    "password = ''\n",
    "auth = (user, password) # For testing only. Don't store credentials in code.\n",
    "\n",
    "# Optional client certificates if you don't want to use HTTP basic authentication.\n",
    "# client_cert_path = '/full/path/to/client.pem'\n",
    "# client_key_path = '/full/path/to/client-key.pem'\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = auth,\n",
    "    # client_cert = client_cert_path,\n",
    "    # client_key = client_key_path,\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    "    #, ca_certs = ca_certs_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f1928-56c6-4343-8119-b268b99c6f28",
   "metadata": {},
   "source": [
    "## Cluster information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8c80dc7-eb1f-4767-9a25-d05ffd546999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------- SERVER INFO\n",
      "{'cluster_name': 'opensearch-cluster',\n",
      " 'cluster_uuid': '6OAGZGbfQgOEPtFM5qLB6Q',\n",
      " 'name': 'opensearch-node1',\n",
      " 'tagline': 'The OpenSearch Project: https://opensearch.org/',\n",
      " 'version': {'build_date': '2022-01-14T03:38:06.881862Z',\n",
      "             'build_hash': 'e505b10357c03ae8d26d675172402f2f2144ef0f',\n",
      "             'build_snapshot': False,\n",
      "             'build_type': 'tar',\n",
      "             'distribution': 'opensearch',\n",
      "             'lucene_version': '8.10.1',\n",
      "             'minimum_index_compatibility_version': '6.0.0-beta1',\n",
      "             'minimum_wire_compatibility_version': '6.8.0',\n",
      "             'number': '1.2.4'}}\n",
      "----------------------------------------------------------------------------------- CLUSTER HEALTH\n",
      "{'active_primary_shards': 94,\n",
      " 'active_shards': 94,\n",
      " 'active_shards_percent_as_number': 83.92857142857143,\n",
      " 'cluster_name': 'opensearch-cluster',\n",
      " 'delayed_unassigned_shards': 0,\n",
      " 'discovered_master': True,\n",
      " 'initializing_shards': 0,\n",
      " 'number_of_data_nodes': 1,\n",
      " 'number_of_in_flight_fetch': 0,\n",
      " 'number_of_nodes': 1,\n",
      " 'number_of_pending_tasks': 0,\n",
      " 'relocating_shards': 0,\n",
      " 'status': 'yellow',\n",
      " 'task_max_waiting_in_queue_millis': 0,\n",
      " 'timed_out': False,\n",
      " 'unassigned_shards': 18}\n",
      "\n",
      "----------------------------------------------------------------------------------- CLUSTER INDICES\n",
      "yellow open  security-auditlog-2022.01.28 P7_PbrA6RweryExfwr5CoQ 1 1      121    0 235.2kb 235.2kb\n",
      "yellow open  security-auditlog-2022.01.29 X9PgPf5vSq60R63OT0J_sQ 1 1       69    0 187.5kb 187.5kb\n",
      "yellow open  security-auditlog-2022.03.27 s3SuSCaSSH6xOpVLR4Z3kA 1 1      129    0 218.3kb 218.3kb\n",
      "yellow open  security-auditlog-2022.03.26 7nXh1hAQSEykJ8D2aed8ww 1 1      196    0 266.8kb 266.8kb\n",
      "yellow open  security-auditlog-2022.03.25 XH8uJuSpT1CdgTPzyUEXAQ 1 1      499    0 628.8kb 628.8kb\n",
      "yellow open  security-auditlog-2022.03.24 twmnOfEvSBuKSpMI8luZFw 1 1       61    0 220.2kb 220.2kb\n",
      "yellow open  security-auditlog-2022.03.23 XQDWeMujSiaVnhEJQdS9Ig 1 1      314    0 425.7kb 425.7kb\n",
      "green  open  .opendistro_security         kuEk9WheSHSC7LASwAu8xw 1 0        9   21 153.5kb 153.5kb\n",
      "yellow open  security-auditlog-2022.03.22 eJxsS4ZNTbWd5wBNckdIcg 1 1       94    0 153.2kb 153.2kb\n",
      "yellow open  security-auditlog-2022.03.20 B4jqLe4fReO6pJmHt5W14w 1 1       31    0  88.4kb  88.4kb\n",
      "green  open  user214                      vS7ri59wQoCnCilwkiGRRw 4 0      994    0 347.4kb 347.4kb\n",
      "green  open  user212                      Q7OjQGWwRQmKrDtldTemwA 4 0    34501    0   4.6mb   4.6mb\n",
      "green  open  user213                      S-FuLm5QQi-aNU_U87yrLA 4 0        1    0   4.5kb   4.5kb\n",
      "green  open  user219                      h6f9v3yoQg-T8oeG_xSgNg 4 0        2    0  43.7kb  43.7kb\n",
      "green  close user216                      wfBCQmgHQqeB1ivCSgRAKg 4 0                              \n",
      "green  open  user217                      Tv3gF32-Sd6HTbkxwGL3ng 4 0        0    0    832b    832b\n",
      "green  open  train-index                  Ulkrx_2YTCWwHVguIVT0ww 1 0    38435    0 560.9mb 560.9mb\n",
      "green  open  user210                      GGcTpEWtS3GGP8Xkc9TuDw 4 0        0    0    832b    832b\n",
      "green  open  user211                      zq34o_R-SXynYZgvnIhxSw 4 0        1    0   4.5kb   4.5kb\n",
      "yellow open  security-auditlog-2022.03.19 omUwM3UUQR-aqFjmnAcRhw 1 1       18    0 130.5kb 130.5kb\n",
      "yellow open  security-auditlog-2022.03.18 DAUfYNzgTNOFnG6SqB3Z7Q 1 1      115    0 236.7kb 236.7kb\n",
      "green  open  .kibana_92668751_admin_1     oEk7w67gSjulMIkmjtjbWQ 1 0        1    0     5kb     5kb\n",
      "yellow open  security-auditlog-2022.03.17 or6bmuKPTLai5y7MZAnumQ 1 1       50    0 191.9kb 191.9kb\n",
      "yellow open  security-auditlog-2022.03.16 UWFPXYpLRJKoK7FjXEcUHw 1 1      227    0 312.2kb 312.2kb\n",
      "yellow open  security-auditlog-2022.03.15 hmm-CC-ETk-GGiQm360_Fg 1 1      174    0 214.6kb 214.6kb\n",
      "green  open  user209                      41z7d69ETNyxdH2mDS7Vew 4 0      994  445 651.8kb 651.8kb\n",
      "yellow open  security-auditlog-2022.03.13 OoYbMb6ySTmFxcqjO6kgAQ 1 1       29    0 153.7kb 153.7kb\n",
      "green  open  .kibana_1                    tx2QleSFSi2xvCvRzBS3Fw 1 0        0    0    208b    208b\n",
      "yellow open  security-auditlog-2022.03.11 Ss9uz3Y-QIKwJtx5laBCzg 1 1        9    0 136.6kb 136.6kb\n",
      "green  open  kwiz                         DSI3_MHNTQeStt382oe0YQ 4 0 38429852    0   575gb   575gb\n",
      "yellow open  security-auditlog-2022.01.30 9-I_Mfk7TVyEwEdpXgXDtw 1 1       81    0   137kb   137kb\n",
      "green  open  user203                      JNUlA42fRd29-44HkIJbPQ 4 0        1    0  10.3kb  10.3kb\n",
      "green  open  user204                      6w_3-yAARmWd23xdgvE4AQ 4 0      994    0 327.1kb 327.1kb\n",
      "green  open  user201                      AAYkFtPWRQiqMJW2O7I3VA 4 0     4312 7379   1.9mb   1.9mb\n",
      "green  open  user207                      a_jXHR5mToK48Mr907Dr_A 4 0        1    0   4.5kb   4.5kb\n",
      "green  open  user208                      MH72o7VwRVu2oxvAL5ccvw 4 0      994  994 620.9kb 620.9kb\n",
      "green  open  user205                      N7Fr4bChRsWvCzI6al-vjQ 4 0      994    0 271.7kb 271.7kb\n",
      "green  open  user206                      jSfY9c--Q-Gao1WrjBm7NA 4 0      994    0 260.5kb 260.5kb\n",
      "green  open  user220                      hl1QXgc2QFKHOuZjAJgltQ 4 0        2    0  43.6kb  43.6kb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------------------------------------- SERVER INFO')\n",
    "response = client.info()\n",
    "pp.pprint(response)\n",
    "\n",
    "print('----------------------------------------------------------------------------------- CLUSTER HEALTH')\n",
    "response = client.cluster.health()\n",
    "pp.pprint(response)\n",
    "\n",
    "print('\\n----------------------------------------------------------------------------------- CLUSTER INDICES')\n",
    "response = client.cat.indices()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3272fd-09d7-4b97-9cda-3bcecbb489f1",
   "metadata": {},
   "source": [
    "## REST Connection to server\n",
    "\n",
    "The REST API should be used for requests that are not available in the Python API. You need to read the OpenSearch/ElasticSearch documentation carefully before using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640c593-b97b-4195-8d49-ee4972bd75e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "s = requests.Session()\n",
    "s.auth = auth\n",
    "\n",
    "#auth = (index_name, 'zya*xJ!4]n') # For testing only. Don't store credentials in code.\n",
    "ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA.\n",
    "server_uri = 'https://' + host + ':' + str(port)\n",
    "\n",
    "# function for the REST requests\n",
    "def opensearch_REST(uri = '/' , body='', verb='get'):\n",
    "    # pass header option for content type if request has a\n",
    "    # body to avoid Content-Type error in Elasticsearch v6.0\n",
    "    \n",
    "    uri = server_uri + uri\n",
    "    print(uri)\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # make HTTP verb parameter case-insensitive by converting to lower()\n",
    "        if verb.lower() == \"get\":\n",
    "            resp = s.get(uri, json=body, headers=headers, verify=False)\n",
    "        elif verb.lower() == \"post\":\n",
    "            resp = s.post(uri, json=body, headers=headers, verify=False)\n",
    "        elif verb.lower() == \"put\":\n",
    "            resp = s.put(uri, json=body, headers=headers, verify=False)\n",
    "        elif verb.lower() == \"del\":\n",
    "                resp = s.delete(uri, json=body, headers=headers, verify=False)\n",
    "        elif verb.lower() == \"head\":\n",
    "                resp = s.head(uri, json=body, headers=headers, verify=False)\n",
    "\n",
    "        # read the text object string\n",
    "        try:\n",
    "            resp_text = json.loads(resp.text)\n",
    "        except:\n",
    "            resp_text = resp.text\n",
    "\n",
    "        # catch exceptions and print errors to terminal\n",
    "    except Exception as error:\n",
    "        print ('\\nelasticsearch_curl() error:', error)\n",
    "        resp_text = error\n",
    "\n",
    "    # return the Python dict of the request\n",
    "    return resp_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP & IR",
   "language": "python",
   "name": "nlp-ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
