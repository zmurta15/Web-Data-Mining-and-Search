{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618a88c7-ea1f-4b1a-8d90-9124f7174ddb",
   "metadata": {},
   "source": [
    "# Fine-tuning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c86e9f-ac9e-492a-8e83-db9f50ca839a",
   "metadata": {},
   "source": [
    "## Masked Language Model\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/auto#transformers.AutoModelForMaskedLM\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b2cb8-8ca6-4cc3-9802-66f37e04479e",
   "metadata": {},
   "source": [
    "## Next Sentence Prediction\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/auto#transformers.AutoModelForNextSentencePrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65333bb5-b498-4f90-81c0-63ff202cab17",
   "metadata": {},
   "source": [
    "## Sequence classification\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72535acb-d28d-4f0b-b49b-52acd9e72ee6",
   "metadata": {},
   "source": [
    "## Token classification\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/auto#transformers.AutoModelForTokenClassification\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575088d0-4bc7-435e-96c8-98f24a627074",
   "metadata": {},
   "source": [
    "\n",
    "## Question-Answering\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/auto#transformers.AutoModelForQuestionAnswering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abbc38-6fed-44b4-9224-ca05935324d3",
   "metadata": {},
   "source": [
    "# Fine-tuning a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea2cd46-5e4c-453c-bbef-69f3b3411765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import transformers\n",
    "from datasets import load_metric\n",
    "\n",
    "from dataset_loader import IntentDataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7d77de-a96c-43da-973e-9185e596ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.logging.set_verbosity_info()\n",
    "transformers.logging.set_verbosity_error() \n",
    "# We set the verbosity to error to avoid the annoying huggingface warnings \n",
    "# when loading models before training them. If you're having trouble getting things to work\n",
    "# maybe comment that line (setting the verbosity to info also may lead to interesting outputs!)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\" # trainer (?) was complaining about parallel tokenization\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" # trainer was complaining about wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd68b70-cff3-4993-9b8b-06d2d670df4b",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d62015d-faa8-452f-a1bd-63da4f88b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = 'roberta-base' # try 'bert-base-uncased', 'bert-base-cased', 'bert-large-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name) # loads a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95209b6c-bf8c-4200-b4e8-5f21e62cf5a8",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f8d2cc-dd40-4d9f-8c6e-1fba983a8d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Intent detection dataset. 5916 examples. (train). \n",
      "Loaded Intent detection dataset. 819 examples. (val). \n",
      "Loaded Intent detection dataset. 842 examples. (test). \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'twiz-data' # rename to your dataset dir\n",
    "\n",
    "train_dataset = IntentDataset(dataset_name, tokenizer, 'train') # check twiz_dataset.py for dataset loading code\n",
    "val_dataset = IntentDataset(dataset_name, tokenizer, 'val')\n",
    "test_dataset = IntentDataset(dataset_name, tokenizer, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31639bd4-0348-4681-a90c-2459b5739554",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d97d9ef-7412-402e-92cb-cf4c666e2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pre_trained = 'roberta-base'\n",
    "model_finetuned = './twiz-intent/checkpoint-555'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_pre_trained, \n",
    "                                                           num_labels=len(train_dataset.all_intents)) # Loads the BERT model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c8d0c-26d4-4475-bd37-a3e0186ad5bd",
   "metadata": {},
   "source": [
    "## Inspect a data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14adcad7-37ea-480d-85f4-f69e2ea1d431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data keys: dict_keys(['input_ids', 'attention_mask', 'label'])\n",
      "\n",
      "INPUT:  <s>Please be careful when using any tools or equipment. Remember, safety first! Here is some information about Bacon and Tomato Pasta. It has a 4.8 star rating.  It is estimated to take about 35 minutes. It serves 4. Its difficulty level is Easy.  If this is not quite what you are looking for say, go back. To continue the task, just say, show ingredients.</s></s>show ingredients</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "INTENT:  tensor(29)   IngredientsConfirmationIntent\n"
     ]
    }
   ],
   "source": [
    "inspect_index = 0\n",
    "\n",
    "print('All data keys:', train_dataset[inspect_index].keys())\n",
    "\n",
    "print()\n",
    "print(\"INPUT: \", tokenizer.decode(train_dataset[inspect_index]['input_ids'])) #train_dataset[inspect_index]['input_ids'].shape\n",
    "\n",
    "# you can check the correspondence of a label by checking the all_intents attribute, as such:\n",
    "print()\n",
    "\n",
    "print(\"INTENT: \", train_dataset[inspect_index]['label'], \" \", train_dataset.all_intents[train_dataset[inspect_index]['label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac57bd4-ba45-46a7-9f97-3b903bb0c1fc",
   "metadata": {},
   "source": [
    "## Training hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dfdea84-22c0-4b2a-860d-f74d61d895aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = load_metric('accuracy')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='twiz-intent',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    metric_for_best_model='accuracy',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    load_best_model_at_end=True,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = acc.compute(predictions=predictions, references=labels)\n",
    "    return accuracy\n",
    "\n",
    "def get_trainer(model):\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "trainer = get_trainer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad424a-d11c-4c97-9706-bcd838c19ced",
   "metadata": {},
   "source": [
    "## Results: Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2838862d-fd04-46d6-a3a3-614bd09edb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 842\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.577387571334839,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 2.9657,\n",
       " 'eval_samples_per_second': 283.913,\n",
       " 'eval_steps_per_second': 9.104}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f77b2-f166-473c-aa78-9af21a80735c",
   "metadata": {},
   "source": [
    "## Model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4246f805-195b-47dd-9216-9eb5a3a0bcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmag/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5916\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 555\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='555' max='555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [555/555 02:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.910700</td>\n",
       "      <td>1.195577</td>\n",
       "      <td>0.752137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.799600</td>\n",
       "      <td>0.885981</td>\n",
       "      <td>0.810745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.568700</td>\n",
       "      <td>0.804752</td>\n",
       "      <td>0.820513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 819\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to twiz-intent/checkpoint-185\n",
      "Configuration saved in twiz-intent/checkpoint-185/config.json\n",
      "Model weights saved in twiz-intent/checkpoint-185/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 819\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to twiz-intent/checkpoint-370\n",
      "Configuration saved in twiz-intent/checkpoint-370/config.json\n",
      "Model weights saved in twiz-intent/checkpoint-370/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 819\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to twiz-intent/checkpoint-555\n",
      "Configuration saved in twiz-intent/checkpoint-555/config.json\n",
      "Model weights saved in twiz-intent/checkpoint-555/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from twiz-intent/checkpoint-555 (score: 0.8205128205128205).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=555, training_loss=1.0929743036493524, metrics={'train_runtime': 180.1709, 'train_samples_per_second': 98.506, 'train_steps_per_second': 3.08, 'total_flos': 1642190814483840.0, 'train_loss': 1.0929743036493524, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3875c-3df9-4b94-a22b-78db103cb35b",
   "metadata": {},
   "source": [
    "## Results: Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83017594-7507-4292-8176-5d7a4c169d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 842\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5670531988143921,\n",
       " 'eval_accuracy': 0.8634204275534442,\n",
       " 'eval_runtime': 2.3037,\n",
       " 'eval_samples_per_second': 365.504,\n",
       " 'eval_steps_per_second': 11.72,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the next cell with the next line uncommented and fill your checkpoint directory to evaluate the model\n",
    "\n",
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP & IR",
   "language": "python",
   "name": "nlp-ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
